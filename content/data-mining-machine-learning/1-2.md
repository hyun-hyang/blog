---
title: Linear Regression
date: 2023.02.13
slug: 1-2
category: 1. Supervised Learning Linear Regression
---

## Introduction

- Labeled data (x,y) &rightarrow; Supervised Learning 지도학습
- **Predict** (continuous) real-valued output (y) &rightarrow; Regression 변수와의 관계
- Learning model: fitting straight line &rightarrow; Linear Regression
- Training dataset (example)
  - x = feature (input), y = Label (Target)(output)
  - M = total # of training examples
  
## Framework

- $f_\theta (x)$ close to $y$ for each $(x^{(m)}, y^{(m)})$
- Error를 최소화

## Specifying Model's Parameters

- Let: $\theta_0 = 0$
- __Mapping Function__ : $ f_\theta = \theta_1\;x $
- &rightarrow; __MSE Cost Function__ (Error의 평균): $J(\theta_1) = \frac{1}{2M}(\sum_{m=1}^{M}(y^{(m)} - \theta_1\;x^{(m)})^2$
- Error = $y^{(m)} - \theta_1\;x^{(m)}$
- $J(\theta_1)$ = Error의 평균 = $\frac{1}{2M}(\sum_{m=1}^{M}(Error)^2$
- error의 평균의 최솟값으로 다가가는 곳을 찾기, 해당 $\theta_1$이 최적.

<img src = "https://user-images.githubusercontent.com/51802020/219168577-38c69291-4c9c-4ac2-9b19-5c09b87db214.png" width = 500>
  
## Gradient Descent

- 함수 $J$ : 최솟값에 다다를때까지 $\theta_0$과 $\theta_1$을 계속 바꾸어 가며 $J$를 감소시킨다.
- 다른 시작점에서 시작한다면 &rightarrow; 서로 다른 최솟값에 도달한다 &rightarrow; local minimum
- Linear regression cost function이 볼록한 경우 오로지 1개의 최솟값을 갖는다.

- __알고리즘__: 

## Gradient Descent for Linear Regression

## Gradient Descent Convergence

## Multiple Features

## Feature Scaling

## Polynomial Regression

## Dataset and HPO

## Learning Curves

## Regularization

### Big Picture
